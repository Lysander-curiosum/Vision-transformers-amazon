Results Analysis for Inferentia and GPU experiments:

For the runs on Inferentia, the models were compiled on an Inf.2x large instance (4 Neuron cores) with the Ubuntu 20.04 deep learning AMI, 
using Pytorch version 1.11.0, and the Neuron version 1.11.0.2.3.0.0. The Neuron driver version was 1.19.3. 
  
The GPU runs were hosted on a G5.xl instance. The Pytorch version was 1.13.0, tracking the nightly binaries for cuda 11.6. 
As described in the readme, one can install using the following command:

pip install torch --pre --extra-index-url https://download.pytorch.org/whl/nightly/cu116

ViT:

  On Inferentia, the lowest latency predictably occurred with a batchsize of 1 and a pipline size of 1 (utilizing 1 Neuron core). 
  The p90 statistic was 9.007 ms. The lowest, p90 latency was lower on the G5 instance, with batch size 1 achieving 7.14 ms. This would make sense,
  as the G5 instance has more raw processing power, so the time for single operations is faster. 
  Even so, the Inferentia statistic in this regard was comparable, lagging behind by less than 2 ms. 
  
  In terms of maximum throughput, on Inferentia the ViT using 1 Neuroncore with a batchsize of 3 achieved an average throughput of 536.718 
  operations in 1 second. 
  On GPU, the largest batch size that was achievable without memory allocation failures was 128, which had a throughput 
  average of 375.89 operations in one second. 
  Thus, in terms of operations per second, the parallelization capabilities of the Inf.2x instance shine through, with 160 more operations per second in the best run than the best throughput on the G5. 
  As such, ViT model achieved a lower cost per 1m on Inferentia than on the G5 instance, with 0.19$ on the Inf.2x instance and $0.74 on the G5 instance. 
  G5 is 75% more expensive. Thus, working with these
  statistics alone, Inferentia appears to be a competitive option for the deployment of the ViT model.
  
  The ViT was pretrained on the ImageNet dataset. 
  Thus, to test the accuracy of the models on Inferentia and the G5 instance, the validation set from
  ImageNet was used. Both runs used batch sizes of 1. 
  ViT achieved a top-1 accuracy of 76.87% on the Inf.2x instance. To account for the loss in accuracy due to optimization, the accuracy test was also run on a model that had been compiled with --fast-math = none (optimizations disregarded). 
  This model achieved a slightly better top-1 accuracy of 76.92%. 
  These optimizations had significant effect on the latency, raising the p90 statistic to 15.4 ms (vs 9.007) for batch size 1 model. The option was also used with the maximum throughput model, and the throughput dropped from 536.18 to 265.85. 
  The slight bump in accuracy does not appear to be worth the loss in efficiency, which is quite substantial for throughput and latency.
  
  On the G5.xl, ViT achieved accuracy of top-1 79.93%. So, the gpu run had more precise results, but at high cost. 
  A 75% price difference makes the 3.06% accuracy difference look negligible. 
  In terms of time to complete the accuracy runs themselves, the GPU run took 53 seconds, while the Inf.2x large run took 37 seconds. 
  
  Comparing the ViT another industry-relevant model, the Resnet50 architecture was also compiled and ran on the G5 instance and on the Inf.2x instance.
  The accuracy achieved on the ImageNet validation by Resnet50 on Inferentia was 76.38%, and on the G5 instance an accuracy of 76.72%. So The ViT model was more precise both on Inferentia and GPU. 
  
  On the ImageNet dataset in the original paper, ViT-base achieved a top-1 accuracy of 77.91%
 
 

BEiT:

 For BEiT, the lowest latency was also with a batchsize 1 and a pipline size of 1 (utilizing 1 Neuron core). 
  The p90 statistic was 37.568 ms. The lowest p90 latency was lower on the G5 instance, with batch size 1 achieving 8.86 ms. 
  The Inferentia statistic lagged behind considerably for the BEiT model, perhaps in the masked image modeling process (the primary implementation 
  difference from the ViT model). To investigate this further, a deeper look into the model structure is required, and perhaps some assistance 
  from the compiler team to find the operations that are putting large loads on the system, and refactoring the model to remove/refactor these
  burdensome components. 
  
  In terms of maximum throughput, on Inferentia the BEiT using 1 Neuroncore with a batchsize of 4 achieved an average throughput of 154.296 
  operations in 1 second. On GPU, the largest batch size that was achievable without memory allocation failures was 128, which had a throughput 
  average of 355.67 operations in one second. The BEiT model performs far better on the G5 instance than on the Inf.2x instance, in both latency and 
  throughput.
  However, the BEiT model did achieve a lower cost per 1m on Inferentia than on the G5 instance, with 0.65$ on the Inf.2x instance and $0.78 on the G5 instance. 
  
  Similarly to the ViT model, BEiT was pretrained on the ImageNet dataset. 
  Thus, to test the accuracy of the models on Inferentia and the G5 instance, the validation set from ImageNet was used.
  Both runs used batch sizes of 1 for maximum precision. 
  BEiT achieved an accuracy of 83.95% on the Inf.2x instance. The accuracy test was also run on a model that had been compiled with --fast-math = none (optimizations disregarded). 
  This model achieved a slightly better accuracy of %83.97. 
  These optimizations had significant effect on the latency, raising the p90 statistic to 49 ms (vs 37.568) for batch size 1 model.
  The option was also used with the maximum throughput model, and the throughput dropped from 154.296 to 120. 
  The slight bump in accuracy does not appear to be worth the loss in efficiency, which is quite substantial for throughput and latency.
  
  On the G5.xl, BEiT achieved accuracy of 83.96%. 
  Thus accuracies were essentially the same, with Inferentia costing $0.13 per 1 million inferences. 
  Thus, for BEiT, Inferentia is a competitive option. The G5 instance was more time efficient, completing the accuracay run in 56 seconds, while the Inf.2x large run took 129 seconds. 
  
  Comparing the BEiT another industry-relevant model, the Resnet50 architecture was also compiled and ran on the G5 instance and on the Inf.2x instance.
  The accuracy achieved on the ImageNet validation by Resnet50 on Inferentia was 76.38%, and on the G5 instance an accuracy of 76.72%. So The BEiT model was more precise both on Inferentia and GPU. 
  
  In the original paper, BEiT recorded a top-1 accuracy of 83.2% on the ImageNet dataset. 

LayoutLMv3: 
  (model compiles, with serious amounts of fragmentation)
  
  The model does compile with the Neuron Trace call, but it is very fragmented. 
  The model does not compile naturally due to the shape of the inputs in the Hugging Face model. 
  https://huggingface.co/docs/transformers/model_doc/layoutlmv3
  
  A functional wrapper was implemented to allow the model to compile, which involved breaking apart a dictionary input. 
  In the initial compile with the functional wrapper, there were 3 unsupported operations, amax, embedding, and repeat. As such, the model was very fragmented, with 15 recursive script modules. Predictably, when benchmarking the initial model, the latency and throughput were abysmal. The initial model achieved a p90 latency of 283.6 ms, with batch size and a pipeline size of 1. The highest average throughput was 9.829 operations per second, with a pipeline size of 1 and a batch size of 2. The batch size of 2 was the largest model that was able to compile without memory allocation errors. The highest throughput achieved a cost per 1m instances of $10.23. 
  
  Upon further investigation, the embdedding operation is supported by Inferentia. It can be forced onto the accelerator. When compiled with the embedding operation forced onto the accelerator, there were only 13 recursive script modules. The affect of this recompilation on the efficiency of the model was relatively minimal. The lowest p90 latency was 270.339 ms, and the highest throughput was 9.329 operations per second (actually less efficient). The lowest cost per 1m instances was $10.88. The latency did decrease, but the throughput and cost per 1m instances had no positive improvement. The decrease in performance could have been due to a variety of factors, but it was most likely due to outlying problems with the compilation rather than the decreased fragmentation of the model. So the majority of the issues lie with the other two operations, amax and repeat specifically. A ticket has been opened for the Kaena compiler team referring to these issues.
  
  To get a better idea of the idealized performance of the model, the experiments were also run on GPU.
  
  (compare to GPU)


DETR (not compiled)

TimeSformer (not compiled)
