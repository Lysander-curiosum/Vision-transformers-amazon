{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cab481f0",
   "metadata": {},
   "source": [
    "# Compiling and Benchmarking the BEiT model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb15d1db",
   "metadata": {},
   "source": [
    "### In this notebook, we will run through the compilation and benchmarking code for the BEiT model. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d4ffaa",
   "metadata": {},
   "source": [
    "##### The first step is to compile the model for inferentia. Here are the necessary imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96552918",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BeitFeatureExtractor, BeitForImageClassification\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "import neuronperf.torch\n",
    "import torch_neuron\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4753c556",
   "metadata": {},
   "source": [
    "#### Then we load the model from Hugging Face. https://huggingface.co/docs/transformers/model_doc/beit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a18b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractor = BeitFeatureExtractor.from_pretrained(\"microsoft/beit-base-patch16-224\")\n",
    "model = BeitForImageClassification.from_pretrained(\"microsoft/beit-base-patch16-224\")\n",
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e3b000d",
   "metadata": {},
   "source": [
    "These experiments were initially ran on an Inf.2x large instance. As such, the benchmarking was compiled with pipeline sizes of 1 and 4, with batch sizes of 1 through 10. The training loop is simple, compiling the models to the directory that this notebook exists in. Changing the file path will affect the benchmarking script below. This part of the script can be run in the notebook, but a lot of efficiency is lost. It is reccommended to run the beitCompile.py file from the compile folder locally on your instance rather than in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc0eecb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_sizes = [1, 4]\n",
    "batch_sizes = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "\n",
    "for ncp in pipeline_sizes:\n",
    "    for bs in batch_sizes:\n",
    "        model_file = f\"beit_neuron_ncp{ncp}_bs{bs}.pt\"\n",
    "        inputs = torch.randn(bs, 3, 224, 224)\n",
    "        print(f\"ncp: {ncp}  bs: {bs}\")\n",
    "\n",
    "        if not os.path.exists(model_file):\n",
    "            print(\"Attempting model compilation\")\n",
    "            nmod = torch.neuron.trace(model, example_inputs=inputs, compiler_args=['--neuroncore-pipeline-cores', f\"{ncp}\"], strict=False)\n",
    "            nmod.save(model_file)\n",
    "            del(nmod) # we need to release the model from memory so it doesn't affect benchmarking later on\n",
    "        else:\n",
    "            print(f\"Found previously compiled model. Skipping compilation\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2639e788",
   "metadata": {},
   "source": [
    "#### The models have been compiled in the folder that this notebook exists in. Now we can commence the benchmarking. We iterate throguh the pipline sizes and the batch sizes, creating separate csv files for each size. For more information on the details of the neuronperf library, please see the documentation here. https://awsdocs-neuron.readthedocs-hosted.com/en/latest/neuron-guide/neuronperf/index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a84cea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ncp in pipeline_sizes:\n",
    "    for bs in batch_sizes:\n",
    "        model_file = f\"beit_neuron_ncp{ncp}_bs{bs}.pt\"\n",
    "        report_file = f\"beit_neuron_ncp{ncp}_bs{bs}_benchmark.csv\"\n",
    "        inputs = torch.randn(bs, 3, 224, 224)\n",
    "        print(f\"ncp: {ncp}  bs: {bs}\")\n",
    "\n",
    "        if not os.path.exists(report_file):\n",
    "            reports = neuronperf.torch.benchmark(model_filename=model_file, inputs=inputs, batch_sizes=[bs], pipeline_sizes=[ncp])\n",
    "            neuronperf.print_reports(reports)\n",
    "            neuronperf.write_csv(reports, report_file)\n",
    "        else:\n",
    "            print(f\"Report file {report_file} already exists. Skipping this benchmark run.\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb80c6f",
   "metadata": {},
   "source": [
    "### You can aggregate the beit data frames from the compilation here, and look at your results in an organized fashion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d79053",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from glob import glob\n",
    "\n",
    "\"\"\"\n",
    "For sorting through the created dataframes\n",
    "\"\"\"\n",
    "\n",
    "dataframes = []\n",
    "\n",
    "for csv in glob(\"beit*.csv\"):\n",
    "    dataframes.append(pd.read_csv(csv))\n",
    "    print(csv)\n",
    "\n",
    "aggr_df = pd.concat(dataframes)\n",
    "aggr_df\n",
    "\n",
    "# Lowest p90 latency\n",
    "lowest_cost = aggr_df.sort_values(by='latency_p90', ascending=True)[0:5]\n",
    "\n",
    "# Cheapest Price\n",
    "lowest_price = aggr_df.sort_values(by='cost per 1 m instances', ascending=True)[0:5]\n",
    "\n",
    "# Highest average throughput\n",
    "highest_throughput = aggr_df.sort_values(by='throughput average', ascending=False)[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b0f5cfe",
   "metadata": {},
   "source": [
    "To save the dataframes. You can specify the filepath further if you wish to store these files in a directory other than the current one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c60c50ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "lowest_cost.to_csv('beit_lowest_latency.csv')\n",
    "lowest_price.to_csv('beit_lowest_latency.csv')\n",
    "highest_throughput.to_csv('beit_lowest_latency.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18614feb",
   "metadata": {},
   "source": [
    "For accuracy, one can run the inferentiaAccuracy.py files from the Inferentia folder, or use the cell below. The accuracy is simply a run on the the ImageNet dataset, looking at the max softmax values, and considering whether or not the predictions are correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6790775a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from einops import rearrange, reduce\n",
    "import torchvision.transforms as transforms\n",
    "import h5py\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.autograd as autograd\n",
    "from torch.optim import AdamW\n",
    "from datasets import load_dataset, load_metric\n",
    "import numpy as np\n",
    "import tarfile\n",
    "import imageNet\n",
    "import torchvision.datasets\n",
    "from datasets import load_dataset\n",
    "from PIL import Image\n",
    "from einops import repeat, rearrange\n",
    "from torchvision.io import read_image\n",
    "from pathlib import Path\n",
    "import torch.neuron\n",
    "\n",
    "# import the modules\n",
    "import os\n",
    "from os import listdir\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "The purpose of this file is to test the accuracy of the compiled vision transformer models on Inferentia.\n",
    "In the original formulation, the ImageNet validation set was used, the labels of which are included in the text file within this folder.\n",
    "Other datasets may be used, but there will have to be some alterations made to this file. \n",
    "\"\"\"\n",
    "\n",
    "# Here, if you have compiled the model into the current directory, you should\n",
    "# be able to load it directly using the name of the file without a complex filepath\n",
    "model = torch.jit.load('beit_neuron_ncp1_bs1_fast_math.pt')\n",
    "\n",
    "\n",
    "# Labels\n",
    "text_file = open(\"Vision-transformers-amazon/Inferentia/val_map.txt\", \"r\")\n",
    "labels = text_file.readlines()\n",
    "labels_final = []\n",
    "for l in labels:\n",
    "    temp = l.split(' ')[1:2]\n",
    "    add = temp[0].split('\\n')[0]\n",
    "    labels_final.append(int(add))\n",
    "\n",
    "text_file.close()\n",
    "\n",
    "# transform the image for input\n",
    "my_transforms = transforms.Compose([\n",
    "transforms.ToPILImage(),                                                                                                                                                                                                   \n",
    "transforms.Resize((224, 224)),                                                                                                  \n",
    "transforms.ToTensor()                                                                                                           \n",
    "])\n",
    "\n",
    "all_input_images = []\n",
    "num_images = 10000\n",
    "\n",
    "\n",
    "image_index = 1\n",
    "print(\"Preparing images\")\n",
    "\n",
    "def getDigits(image_index):\n",
    "    \"\"\"\n",
    "    The purpose of this function is to read from the ImageNet validation set labels. Ignore if a different dataset is being used. \n",
    "    \"\"\"\n",
    "    length = len(str(image_index))\n",
    "    zeros = (8 - length) * '0'\n",
    "    return zeros + str(image_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd06aae1",
   "metadata": {},
   "source": [
    "Here, in the read_image line, you have to insert your path to the validation dataset. The code will work without hiccup if you load the tar file into this directory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9eeddab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation Input\n",
    "for x in range(num_images):\n",
    "    \"\"\"\n",
    "    You will have to download the imagenet dataset to your machine or instance, and place in preferrably in this folder to make it accessible.\n",
    "    If another dataset is to be used, you may change this line accordingly.\n",
    "    \"\"\"\n",
    "    img =  read_image('Your-path-to-imagenet-dataset/ILSVRC2012_val_' + getDigits(image_index) + '.JPEG')\n",
    "    print(image_index)\n",
    "    toAdd = my_transforms(img)\n",
    "    toAdd = toAdd[None, :, :, :]\n",
    "    if(toAdd.shape != torch.Size([1, 3, 224, 224])):\n",
    "        greyscale.append(image_index)\n",
    "        toAdd = repeat(toAdd, 'b c h w -> b (3 c) h w')\n",
    "    all_input_images.append(toAdd)\n",
    "    image_index+=1\n",
    "\n",
    "model.eval()\n",
    "\n",
    "tot = 0\n",
    "correct = 0\n",
    "index = 0\n",
    "print(\"Evaluation begins\")\n",
    "for m in range(len(huh)):\n",
    "\n",
    "    max_vals = torch.max(model(all_input_images[m])['logits'], dim = -1).indices\n",
    "    print(index)\n",
    "    if(max_vals[0].item() == labels_final[index]):\n",
    "        correct += 1\n",
    "    tot += 1\n",
    "    index += 1\n",
    "\n",
    "print(f\"eval : {correct/tot}\")\n",
    "                                          "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
