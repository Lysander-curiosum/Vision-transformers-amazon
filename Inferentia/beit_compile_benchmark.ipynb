{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cab481f0",
   "metadata": {},
   "source": [
    "# Compiling and Benchmarking the BEiT model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb15d1db",
   "metadata": {},
   "source": [
    "### In this notebook, we will run through the compilation and benchmarking code for the BEiT model. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d4ffaa",
   "metadata": {},
   "source": [
    "##### The first step is to compile the model for inferentia. Here are the necessary imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96552918",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BeitFeatureExtractor, BeitForImageClassification\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "import neuronperf.torch\n",
    "import torch_neuron\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4753c556",
   "metadata": {},
   "source": [
    "#### Then we load the model from Hugging Face. https://huggingface.co/docs/transformers/model_doc/beit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a18b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractor = BeitFeatureExtractor.from_pretrained(\"microsoft/beit-base-patch16-224\")\n",
    "model = BeitForImageClassification.from_pretrained(\"microsoft/beit-base-patch16-224\")\n",
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e3b000d",
   "metadata": {},
   "source": [
    "These experiments were initially ran on an Inf.2x large instance. As such, the benchmarking was compiled with pipeline sizes of 1 and 4, with batch sizes of 1 through 10. The training loop is simple, compiling the models to the directory that this notebook exists in. Changing the file path will affect the benchmarking script below. This part of the script can be run in the notebook, but a lot of efficiency is lost. It is reccommended to run the beitCompile.py file from the compile folder locally on your instance rather than in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc0eecb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_sizes = [1, 4]\n",
    "batch_sizes = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "\n",
    "for ncp in pipeline_sizes:\n",
    "    for bs in batch_sizes:\n",
    "        model_file = f\"beit_neuron_ncp{ncp}_bs{bs}.pt\"\n",
    "        inputs = torch.randn(bs, 3, 224, 224)\n",
    "        print(f\"ncp: {ncp}  bs: {bs}\")\n",
    "\n",
    "        if not os.path.exists(model_file):\n",
    "            print(\"Attempting model compilation\")\n",
    "            nmod = torch.neuron.trace(model, example_inputs=inputs, compiler_args=['--neuroncore-pipeline-cores', f\"{ncp}\"], strict=False)\n",
    "            nmod.save(model_file)\n",
    "            del(nmod) # we need to release the model from memory so it doesn't affect benchmarking later on\n",
    "        else:\n",
    "            print(f\"Found previously compiled model. Skipping compilation\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2639e788",
   "metadata": {},
   "source": [
    "#### The models have been compiled in the folder that this notebook exists in. Now we can commence the benchmarking. We iterate throguh the pipline sizes and the batch sizes, creating separate csv files for each size. For more information on the details of the neuronperf library, please see the documentation here. https://awsdocs-neuron.readthedocs-hosted.com/en/latest/neuron-guide/neuronperf/index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a84cea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ncp in pipeline_sizes:\n",
    "    for bs in batch_sizes:\n",
    "        model_file = f\"beit_neuron_ncp{ncp}_bs{bs}.pt\"\n",
    "        report_file = f\"beit_neuron_ncp{ncp}_bs{bs}_benchmark.csv\"\n",
    "        inputs = torch.randn(bs, 3, 224, 224)\n",
    "        print(f\"ncp: {ncp}  bs: {bs}\")\n",
    "\n",
    "        if not os.path.exists(report_file):\n",
    "            reports = neuronperf.torch.benchmark(model_filename=model_file, inputs=inputs, batch_sizes=[bs], pipeline_sizes=[ncp])\n",
    "            neuronperf.print_reports(reports)\n",
    "            neuronperf.write_csv(reports, report_file)\n",
    "        else:\n",
    "            print(f\"Report file {report_file} already exists. Skipping this benchmark run.\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb80c6f",
   "metadata": {},
   "source": [
    "### You can aggregate the beit data frames from the compilation here, and look at your results in an organized fashion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d79053",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from glob import glob\n",
    "\n",
    "\"\"\"\n",
    "For sorting through the created dataframes\n",
    "\"\"\"\n",
    "\n",
    "dataframes = []\n",
    "\n",
    "for csv in glob(\"beit*.csv\"):\n",
    "    dataframes.append(pd.read_csv(csv))\n",
    "    print(csv)\n",
    "\n",
    "aggr_df = pd.concat(dataframes)\n",
    "aggr_df\n",
    "\n",
    "# Lowest p90 latency\n",
    "lowest_cost = aggr_df.sort_values(by='latency_p90', ascending=True)[0:5]\n",
    "\n",
    "# Cheapest Price\n",
    "lowest_price = aggr_df.sort_values(by='cost per 1 m instances', ascending=True)[0:5]\n",
    "\n",
    "# Highest average throughput\n",
    "highest_throughput = aggr_df.sort_values(by='throughput average', ascending=False)[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b0f5cfe",
   "metadata": {},
   "source": [
    "To save the dataframes. You can specify the filepath further if you wish to store these files in a directory other than the current one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c60c50ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "lowest_cost.to_csv('layout_lowest_latency.csv')\n",
    "lowest_price.to_csv('layout_lowest_latency.csv')\n",
    "highest_throughput.to_csv('layout_lowest_latency.csv')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
