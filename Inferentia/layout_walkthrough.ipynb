{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cab481f0",
   "metadata": {},
   "source": [
    "# Compiling and Benchmarking the BEiT model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb15d1db",
   "metadata": {},
   "source": [
    "### In this notebook, we will run through the compilation and benchmarking code for the BEiT model. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d4ffaa",
   "metadata": {},
   "source": [
    "##### The first step is to compile the model for inferentia. Here are the necessary imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96552918",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoProcessor, AutoModel\n",
    "from datasets import load_dataset\n",
    "import neuronperf.torch\n",
    "import torch_neuron\n",
    "import os\n",
    "import torch\n",
    "from torch import nn, tensor\n",
    "from einops import repeat, rearrange"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4753c556",
   "metadata": {},
   "source": [
    "#### Then we load the model from Hugging Face. https://huggingface.co/docs/transformers/model_doc/beit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a18b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = AutoProcessor.from_pretrained(\"microsoft/layoutlmv3-base\", apply_ocr=False)\n",
    "model = AutoModel.from_pretrained(\"microsoft/layoutlmv3-base\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e3b000d",
   "metadata": {},
   "source": [
    "These experiments were initially ran on an Inf.2x large instance. As such, the benchmarking was compiled with pipeline sizes of 1 and 4, with batch sizes of 1 through 10. The training loop is simple, compiling the models to the directory that this notebook exists in. Changing the file path will affect the benchmarking script below. This part of the script can be run in the notebook, but a lot of efficiency is lost. It is reccommended to run the layoutCompile.py file from the compile folder locally on your instance rather than in the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "144c60f6",
   "metadata": {},
   "source": [
    "For layoutLMv3, the model has an input structure which is not compatible with the Neuron trace call. As such, we have to implement a simple function wrapper which allows the trace call to commence. In this instance, it is as simple as breaking down a dictionary input into its readable components for the actual model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1735a3e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuronCompatibilityWrapper(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super(NeuronCompatibilityWrapper, self).__init__()\n",
    "        self.model = model\n",
    "\n",
    "    def forward(self, encoding):\n",
    "        out = self.model(input_ids = encoding['input_ids'], attention_mask = encoding['attention_mask'], bbox= encoding['bbox'], pixel_values = encoding['pixel_values'])\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3be8d23e",
   "metadata": {},
   "source": [
    "Declare the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa59837",
   "metadata": {},
   "outputs": [],
   "source": [
    "layout_for_trace = NeuronCompatibilityWrapper(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0014772",
   "metadata": {},
   "source": [
    "For the input, we use the example data from the Hugging Face docs. We run the input through the processor. \n",
    "https://huggingface.co/docs/transformers/model_doc/layoutlmv3#transformers.LayoutLMv3Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e7a8ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"nielsr/funsd-layoutlmv3\", split=\"train\")\n",
    "example = dataset[0]\n",
    "image = example[\"image\"]\n",
    "words = example[\"tokens\"]\n",
    "boxes = example[\"bboxes\"]\n",
    "\n",
    "encoding = processor(image, words, boxes=boxes, return_tensors=\"pt\")\n",
    "\n",
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a310af",
   "metadata": {},
   "source": [
    "For the batch sizes, it is reccomended to test 1 through 10. These are typical in terms of workloads that the model can handle for Inferentia on the Inf.2x instance. The pipeline sizes refer to the neuron cores that are utilized during the runs, extending from 1 core to all 4 available on the Inf.2xl instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a5e162a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_sizes = [1, 4]\n",
    "batch_sizes = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2741eb3f",
   "metadata": {},
   "source": [
    "Here, the example data is enlarged to fit the different batch sizes. We use the repeat operation from the einops library. These are inserted into a dictionary for input, before the compilation commences. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc0eecb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ncp in pipeline_sizes:\n",
    "    for bs in batch_sizes:\n",
    "        model_file = f\"layout_neuron_ncp{ncp}_bs{bs}.pt\"\n",
    "        encoding['input_ids'] = repeat(encoding['input_ids'], 'b h -> (x b) h', x = bs)\n",
    "        encoding['bbox'] = repeat(encoding['bbox'], 'b h w -> (x b) h w', x = bs)\n",
    "        encoding['pixel_values'] = repeat(encoding['pixel_values'], 'b c h w -> (x b) c h w', x = bs)\n",
    "        encoding['attention_mask'] = repeat(encoding['attention_mask'], 'b w -> (x b) w', x = bs)\n",
    "        for_trace_dict = {'input_ids':encoding['input_ids'], 'attention_mask':encoding['attention_mask'], 'bbox':encoding['bbox'],'pixel_values':encoding['pixel_values']}\n",
    "        \n",
    "        print(f\"ncp: {ncp}  bs: {bs}\")\n",
    "\n",
    "        if not os.path.exists(model_file):\n",
    "            print(\"Attempting model compilation\")\n",
    "            nmod = torch.neuron.trace(layout_for_trace, example_inputs=for_trace_dict, compiler_args=['--neuroncore-pipeline-cores', f\"{ncp}\"], strict=False)\n",
    "            nmod.save(model_file)\n",
    "            del(nmod) # we need to release the model from memory so it doesn't affect benchmarking later on\n",
    "        else:\n",
    "            print(f\"Found previously compiled model. Skipping compilation\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2639e788",
   "metadata": {},
   "source": [
    "#### The models have been compiled in the folder that this notebook exists in. Now we can commence the benchmarking. We iterate throguh the pipline sizes and the batch sizes, creating separate csv files for each size. For more information on the details of the neuronperf library, please see the documentation here. https://awsdocs-neuron.readthedocs-hosted.com/en/latest/neuron-guide/neuronperf/index.html\n",
    "\n",
    "(Just a note: the data is refit in the benchmarking runs similarly to the compilation runs above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a84cea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ncp in pipeline_sizes:\n",
    "    for bs in batch_sizes:\n",
    "        model_file = f\"layout_neuron_ncp{ncp}_bs{bs}.pt\"\n",
    "        report_file = f\"layout_neuron_ncp{ncp}_bs{bs}_benchmark.csv\"\n",
    "\n",
    "        model_file = f\"layout_neuron_ncp{ncp}_bs{bs}.pt\"\n",
    "        encoding['input_ids'] = repeat(encoding['input_ids'], 'b h -> (x b) h', x = bs)\n",
    "        encoding['bbox'] = repeat(encoding['bbox'], 'b h w -> (x b) h w', x = bs)\n",
    "        encoding['pixel_values'] = repeat(encoding['pixel_values'], 'b c h w -> (x b) c h w', x = bs)\n",
    "        encoding['attention_mask'] = repeat(encoding['attention_mask'], 'b w -> (x b) w', x = bs)\n",
    "        for_trace_dict = {'input_ids':encoding['input_ids'], 'attention_mask':encoding['attention_mask'], 'bbox':encoding['bbox'],'pixel_values':encoding['pixel_values']}\n",
    "\n",
    "        print(f\"ncp: {ncp}  bs: {bs}\")\n",
    "\n",
    "        if not os.path.exists(report_file):\n",
    "            reports = neuronperf.torch.benchmark(model_filename=model_file, inputs=for_trace_dict, batch_sizes=[bs], pipeline_sizes=[ncp])\n",
    "            neuronperf.print_reports(reports)\n",
    "            neuronperf.write_csv(reports, report_file)\n",
    "        else:\n",
    "            print(f\"Report file {report_file} already exists. Skipping this benchmark run.\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb80c6f",
   "metadata": {},
   "source": [
    "### You can aggregate the beit data frames from the compilation here, and look at your results in an organized fashion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d79053",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from glob import glob\n",
    "\n",
    "\"\"\"\n",
    "For sorting through the created dataframes\n",
    "\"\"\"\n",
    "\n",
    "dataframes = []\n",
    "\n",
    "for csv in glob(\"layout*.csv\"):\n",
    "    dataframes.append(pd.read_csv(csv))\n",
    "    print(csv)\n",
    "\n",
    "aggr_df = pd.concat(dataframes)\n",
    "aggr_df\n",
    "\n",
    "# Lowest p90 latency\n",
    "lowest_cost = aggr_df.sort_values(by='latency_p90', ascending=True)[0:5]\n",
    "\n",
    "# Cheapest Price\n",
    "lowest_price = aggr_df.sort_values(by='cost per 1 m instances', ascending=True)[0:5]\n",
    "\n",
    "# Highest average throughput\n",
    "highest_throughput = aggr_df.sort_values(by='throughput average', ascending=False)[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b0f5cfe",
   "metadata": {},
   "source": [
    "To save the dataframes. You can specify the filepath further if you wish to store these files in a directory other than the current one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c60c50ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "lowest_cost.to_csv('layout_lowest_latency.csv')\n",
    "lowest_price.to_csv('layout_lowest_latency.csv')\n",
    "highest_throughput.to_csv('layout_lowest_latency.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
